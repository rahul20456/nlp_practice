Natural Language Processing<br>
---------------------------
Codes, datasets, and explanations for some basic natural language tasks and models. <br>

This repository is a set of 5 tutorials which provide a basic knowledge about NLP, and applying it to text to get results without needing a deep mathematical knowledge of the working behind the models. NLTK, Keras and sklearn are the main libraries used in the tutorials. Each folder contains the datasets and a Jupyter Notebook for that tutorial. I've also written detailed Medium articles to explain the code in the Notebooks, which are linked below. <br>

1. NLP Preprocessing <br>
Explains the basic preprocessing tasks to be performed before training almost any model. Covers stemming and lemmatization and their differences. <br> 
[Medium article](https://towardsdatascience.com/text-preprocessing-with-nltk-9de5de891658)

2. Language Modeling <br>
Building and studying statistical language models from a corpus dataset. Unigram, bigram, trigram and fourgram models are covered. <br>
[Medium article](https://medium.com/swlh/language-modelling-with-nltk-20eac7e70853)

3. Classifier Models <br>
Building and comparing the accuracy of Naive Bayes and LSTM models on a given dataset using NLTK and Keras. <br>
[Medium article](https://towardsdatascience.com/naive-bayes-and-lstm-based-classifier-models-63d521a48c20)

4. Conditional Random Fields <br>
Experimenting with POS tagging, a standard sequence labeling task using CRFs with the help of the scikit-learn crfsuite wrapper. <br>
[Medium article](https://towardsdatascience.com/pos-tagging-using-crfs-ea430c5fb78b)

5. Word and Character Based LSTM Models <br>
Building and analyzing word and character based LSTM models. Two different character based models are also trained and compared. <br>
[Medium article](https://towardsdatascience.com/word-and-character-based-lstms-12eb65f779c2)
